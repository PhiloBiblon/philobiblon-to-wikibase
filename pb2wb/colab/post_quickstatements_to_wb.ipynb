{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhiloBiblon/philobiblon-to-wikibase/blob/master/pb2wb/colab/post_quickstatements_to_wb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAxWMP15ZofD"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK99N4bLZ1__"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xic-_snBZ5NL"
      },
      "outputs": [],
      "source": [
        "service = build('drive', 'v3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaT5r5mnxxSP"
      },
      "outputs": [],
      "source": [
        "SPLIT_FILES_SOURCE = {'BETA': '187KTNwJ2LZXf5d8WAFO1qnP6g8Yqderw',\n",
        "                      'BITECA': '',\n",
        "                      'BITAGAP': ''}\n",
        "\n",
        "SPLIT_FILES_PROCESSED= {'BETA': '1lc-XTCm9lSAfO6xIxxQ5K_4LzHhj3nPl',\n",
        "                           'BITECA': '',\n",
        "                           'BITAGAP': ''}\n",
        "WB_CONFIGS = {\n",
        "    'pb.cloud': {\n",
        "        'MEDIAWIKI_API_URL': 'https://pbsandbox.wikibase.cloud/qs/api.php',\n",
        "        'WB_USER': 'pb.cloud.user',\n",
        "        'WB_PASSWORD': 'pb.cloud.password',\n",
        "        'WB_TOKEN': 'pb.cloud.token'\n",
        "    },\n",
        "    'pb.cog': {\n",
        "        'MEDIAWIKI_API_URL': \"https://philobiblon.cog.berkeley.edu/qs/api.php\",\n",
        "        'WB_USER': 'pb.cog.user',\n",
        "        'WB_PASSWORD': 'pb.cog.password',\n",
        "        'WB_TOKEN': 'pb.cog.token',\n",
        "    },\n",
        "        'factgrid': {\n",
        "        'MEDIAWIKI_API_URL': \"https://database.factgrid.de/qs/api.php\",\n",
        "        'WB_USER': 'factgrid.user',\n",
        "        'WB_PASSWORD': 'factgrid.password',\n",
        "        'WB_TOKEN': 'factgrid.token',\n",
        "    }\n",
        "}\n",
        "\n",
        "# Manually update bibliography, table and instance that is to be updated\n",
        "bibliography = 'BETA' # BETA BITECA BITAGAP\n",
        "table = 'subject' # 'geography' 'analytic' 'library' 'ms_ed' 'biographies' 'copies' 'institutions' 'subject' 'uniform_title'\n",
        "instance = \"pb.cog\" # pb.cloud pb.cog factgrid\n",
        "batch_id = ''\n",
        "start_time = time.time()\n",
        "elapsed_time = 0\n",
        "max_seconds = 36000\n",
        "complete_status = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFun94tv9TTp"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "URL = WB_CONFIGS[instance]['MEDIAWIKI_API_URL']\n",
        "WB_USER = userdata.get(WB_CONFIGS[instance]['WB_USER'])\n",
        "WB_PASSWORD = userdata.get(WB_CONFIGS[instance]['WB_PASSWORD'])\n",
        "WB_TOKEN = userdata.get(WB_CONFIGS[instance]['WB_TOKEN'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROJUV1wB8cEQ"
      },
      "outputs": [],
      "source": [
        "def time_check():\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "    return elapsed_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeJGaYdvlDQy"
      },
      "outputs": [],
      "source": [
        "def get_batch_status(batch_id):\n",
        "    batch_status_command = f\"curl {URL} -d action=get_batch_info -d batch={batch_id}\"\n",
        "    batch_status = subprocess.run(batch_status_command, capture_output=True, text=True, shell=True)\n",
        "    try:\n",
        "        data = json.loads(batch_status.stdout)\n",
        "        batch_status = data[\"data\"][str(batch_id)][\"batch\"][\"status\"]\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Error parsing JSON output:\", batch_status.stdout)\n",
        "    try:\n",
        "        error_count = data[\"data\"][str(batch_id)][\"commands\"]['ERROR']\n",
        "    except:\n",
        "        error_count = 0\n",
        "    return batch_status, error_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0tPgYSQzTpy"
      },
      "outputs": [],
      "source": [
        "def move_file(source_id, destination_id, file_name, file_id):\n",
        "    file = service.files().get(fileId=file_id, fields='parents').execute()\n",
        "    previous_parents = \",\".join(file.get('parents'))\n",
        "    file = service.files().update(fileId=file_id,\n",
        "                                  addParents=destination_id,\n",
        "                                  removeParents=previous_parents,\n",
        "                                  fields='id, parents').execute()\n",
        "\n",
        "    print(f\"File '{file_name}' moved to folder with ID '{destination_id}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdmASJlTfZJV",
        "outputId": "e25c2813-cb6a-49e0-fae2-1e5fbe85c1cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Items:\n",
            "split_beta_biography_qs_28.qs (1bNVcFkTFa2EzSemZD6XQ1iy5I6ZYG0Aa)\n",
            "split_beta_biography_qs_27.qs (18uue4_gf0Pt5wqSiPOqyaL2PELGzl1zZ)\n",
            "split_beta_biography_qs_26.qs (1Z7TzCjq9we_5rhA19r35BVwUWecoNp15)\n",
            "split_beta_biography_qs_25.qs (1Ipr_SxQ2Km59ZKukwof18VuSMo7sbIoP)\n",
            "split_beta_biography_qs_24.qs (1zlNlSeVHG9QLBetvwPHAsMZSVPxZcXXo)\n",
            "split_beta_biography_qs_23.qs (15lBHWTrfh_tuGjRxRzqYrqLBINtTA18a)\n",
            "split_beta_biography_qs_22.qs (1LsesqMGmkl6f1_XGpG3PfeEHfriXvqc-)\n",
            "split_beta_biography_qs_21.qs (1Z6INXbbslcX1C_JGXZnFBIzbIC9YiM3X)\n",
            "split_beta_biography_qs_20.qs (1n3jg4jk7IXudelDbo7tiKSU8-VQdNwWm)\n",
            "split_beta_biography_qs_19.qs (1WzrHTbwJMwq3lZROSx_w-QtVRrNOxRbg)\n",
            "split_beta_biography_qs_18.qs (1DsWprBcBfG2PVGsK-TM0EES3tVBl10tG)\n",
            "split_beta_biography_qs_17.qs (1YcxzZjxuT3fps5gRJlCkqhmMO7hNa0QU)\n",
            "split_beta_biography_qs_16.qs (10PfXMwHqibvNr3EFBiproCSfFiTXtE6Q)\n",
            "split_beta_biography_qs_15.qs (1fNLHFeISLnzJN8uUjwVXFHzSdDMAtsEe)\n",
            "split_beta_biography_qs_14.qs (1J2RpziyorCDBCWsef2lpeNQ0tEv2aDBf)\n",
            "split_beta_biography_qs_13.qs (1zyhZqBHbQ9VDPPYnwAnSMNgvZD8c4XrB)\n",
            "split_beta_biography_qs_12.qs (1PeDFCousCCS6nGzwYT3Wsg2VuKMcrx7y)\n",
            "split_beta_biography_qs_11.qs (17roqeIbYUqAD7HY0LQMH8qDIgYSoehnX)\n",
            "split_beta_biography_qs_10.qs (1DQeTC96_GxdoPulmQObL5vP6k4UGT6MK)\n",
            "split_beta_biography_qs_9.qs (1YX4Wz40R6Ll7vEDLXL3i1I5ZvPVaNzlX)\n",
            "split_beta_biography_qs_8.qs (1ohdILhiGBsIDTHEAMEP8ro1ezTahxgDi)\n",
            "split_beta_biography_qs_7.qs (1twTiXQAGvvXcP8dZuZg-3BNhHRVLiI-M)\n",
            "split_beta_biography_qs_6.qs (1R1iWUkRbNwPeJdRBFl-CpT_qHLaC8Hoj)\n",
            "split_beta_biography_qs_5.qs (1zAVbasrhiY5og7krA1862oGkNf5AXn1b)\n",
            "split_beta_biography_qs_4.qs (1ctHjYTPNaD-1kfvKzJ_EYf1T_P81N1xu)\n",
            "split_beta_biography_qs_3.qs (1egfhT4-gOhtNbmQfB2h90JysP52S_fUq)\n",
            "split_beta_biography_qs_2.qs (1T27xqGAAN21aWElkGpKn5ETAKGx6_vLh)\n",
            "split_beta_biography_qs_1.qs (1sye829HCOyHTqRzf2kGhNXwckDSXYsbK)\n",
            "split_beta_biography_qs_0.qs (11t78FAX385WlqzkWjTdWO0cMUwLIT4vs)\n",
            "split_beta_subject_qs_0.qs (1lH7rbjTc4Bbs2tNx1ttpoyHOEyTCHxxZ)\n",
            "Split Files Done (1lc-XTCm9lSAfO6xIxxQ5K_4LzHhj3nPl)\n",
            "[{'id': '1lH7rbjTc4Bbs2tNx1ttpoyHOEyTCHxxZ', 'name': 'split_beta_subject_qs_0.qs'}]\n",
            "List of all files to be processed: ['split_beta_subject_qs_0.qs']\n",
            "Elapsed time: 2.3506617546081543 seconds\n",
            "split_beta_subject_qs_0.qs (1lH7rbjTc4Bbs2tNx1ttpoyHOEyTCHxxZ)\n",
            "Downloading file: split_beta_subject_qs_0.qs\n",
            "Download 100%.\n",
            "File 'split_beta_subject_qs_0.qs' downloaded successfully.\n",
            "File 'split_beta_subject_qs_0.qs' moved to folder with ID '1lc-XTCm9lSAfO6xIxxQ5K_4LzHhj3nPl'.\n",
            "Batch ID: 61\n",
            "Batch import 61 still running, sleeping for 10 minutes\n",
            "Checking batch status\n",
            "Batch import 61 still running, sleeping for 10 minutes\n",
            "Checking batch status\n",
            "Batch import 61 complete with status DONE\n",
            "Batch 61 had 3 errors\n",
            "Elapsed time: 1222.07359957695 seconds\n",
            "All files processed.  Completed in: 1222.07359957695 seconds\n"
          ]
        }
      ],
      "source": [
        "path_id = SPLIT_FILES_SOURCE[str(bibliography)]\n",
        "destination_id = SPLIT_FILES_PROCESSED[str(bibliography)]\n",
        "\n",
        "# Check for files in source directory\n",
        "results = service.files().list(q=f\"'{path_id}' in parents and trashed=false\", pageSize=1000, fields=\"nextPageToken, files(id, name)\").execute()\n",
        "items = results.get('files', [])\n",
        "if not items:\n",
        "    print('No items found.')\n",
        "else:\n",
        "    print('Items:')\n",
        "    for item in items:\n",
        "        print(u'{0} ({1})'.format(item['name'], item['id']))\n",
        "\n",
        "item_dict = [item for item in items if isinstance(item, dict)][0]\n",
        "table_items = [item for item in items if item['name'].endswith('.qs') and table in item['name']]\n",
        "print(table_items)\n",
        "\n",
        "# Sort table items by split number\n",
        "table_items.sort(key=lambda x: int(x['name'].split('_')[4].split('.')[0]) if len(x['name'].split('_')) >= 4 else 0, reverse=False)\n",
        "names = [item['name'] for item in table_items]\n",
        "print(f'List of all files to be processed: {names}')\n",
        "\n",
        "# Loop through sorted items and process\n",
        "elapsed_time = time_check() # Get elapsed time so far to start\n",
        "while elapsed_time < max_seconds and not complete_status:\n",
        "    for table_item in table_items:\n",
        "        date = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "        batchname = f'{bibliography}_{table}_{date}'\n",
        "        print(u'{0} ({1})'.format(table_item['name'], table_item['id']))\n",
        "        file_id = table_item['id']\n",
        "        file_name = table_item['name']\n",
        "        # Download the file\n",
        "        print(f'Downloading file: {file_name}')\n",
        "        request = service.files().get_media(fileId=file_id)\n",
        "        fh = io.BytesIO()\n",
        "        downloader = MediaIoBaseDownload(fh, request)\n",
        "        done = False\n",
        "        while done is False:\n",
        "            status, done = downloader.next_chunk()\n",
        "            print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "\n",
        "        # Save the file to your Colab environment\n",
        "        with open(file_name, 'wb') as f:\n",
        "            f.write(fh.getbuffer())\n",
        "            print(f\"File '{file_name}' downloaded successfully.\")\n",
        "\n",
        "        # Run curl command submitting batch\n",
        "        curl_command = f'curl {URL} -d action=import -d submit=1 -d format=v1 -d username={WB_USER} -d batchname={batchname} --data-raw token=\\'{WB_TOKEN}\\' --data-urlencode data@{file_name}'\n",
        "        post_qs = subprocess.run(curl_command, capture_output=True, text=True, shell=True)\n",
        "        time.sleep(10) # Wait for batch to initiate\n",
        "\n",
        "        # move qs file to completed folder after submitted batch\n",
        "        move_file(path_id, destination_id, file_name, file_id)\n",
        "\n",
        "        if post_qs.returncode != 0:\n",
        "            print(\"Error executing curl command:\", post_qs.stderr)\n",
        "            exit(1)\n",
        "        try:\n",
        "            data = json.loads(post_qs.stdout)\n",
        "            batch_id = data[\"batch_id\"]\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error parsing JSON output for {batch_id}:\", post_qs.stdout)\n",
        "            continue\n",
        "        print(f\"Batch ID: {batch_id}\")\n",
        "        status = get_batch_status(batch_id)\n",
        "        while status != \"DONE\":\n",
        "            print(f'Batch import {batch_id} still running, sleeping for 10 minutes')\n",
        "            time.sleep(600)\n",
        "            print(\"Checking batch status\")\n",
        "            status, error_count = get_batch_status(batch_id)\n",
        "        print(f'Batch import {batch_id} complete with status {status}')\n",
        "        print(f'Batch {batch_id} had {error_count} errors')\n",
        "\n",
        "        # update elapsed time\n",
        "        elapsed_time = time_check()\n",
        "\n",
        "    complete_status = True\n",
        "\n",
        "print(f'All files processed.  Completed in: {elapsed_time} seconds')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYSHAWO+Y9B0TKIi1tFUmS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}